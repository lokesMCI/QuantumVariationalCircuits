# -*- coding: utf-8 -*-
"""type_3 environment_CognitiveRadioEnvironment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ao6TRlUqOh5xiP6rC78UaMux_57xhpwo
"""

#import numpy as np

# Importing standard Qiskit libraries
from qiskit import QuantumCircuit, transpile, Aer, IBMQ
from qiskit.tools.jupyter import *
from qiskit.visualization import *
from ibm_quantum_widgets import *
from qiskit.providers.aer import QasmSimulator

# Loading your IBM Quantum account(s)
provider = IBMQ.load_account()

#pip install autoray==0.2.5

pip install gym

pip install pennylane

# Differentiable Quantum programming using pennylane
import pennylane as qml 
from pennylane import numpy as np

import gym





import torch
import torch.nn as nn
from torch.autograd import Variable
from collections import namedtuple
import random

"""from gym.envs.registration import register
'''register(
     id='Deterministic-4x4-FrozenLake-v0', # name given to this new environment
     entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', # env entry point
     kwargs={'map_name': '4x4', 'is_slippery': False} # argument passed to the env
 )'''
register(
    id='Deterministic-ShortestPath-4x4-FrozenLake-v0', # name given to this new environment
    entry_point='ShortestPathFrozenLake:ShortestPathFrozenLake', # env entry point
    kwargs={'map_name': '4x4', 'is_slippery': False} # argument passed to the env
)

### Replay Memory
"""

# Creating Replay memory to store the ('state','action','reward','next_state','done')

Transition = namedtuple('Transition',('state','action','reward','next_state','done'))

class Replay_mem(object):
    def __init__(self,n):
        self.n = n
        self.mem = []
        self.pos = 0
    
    def add(self,*args):
        if len(self.mem)<self.n:
            self.mem.append(None)
        self.mem[self.pos]=Transition(*args)
        self.pos=(self.pos+1)%self.n
        
    def sample(self,batch_size):
        return random.sample(self.mem,batch_size)
    
    def disp(self):
        return self.mem
    
    def __len__(self):
        return len(self.mem)

"""### Embedding Classical states to Qubits using " Basis Embedding """"

# Converting the state(in decimal) to fixed length binary representation
def encoding(_len,_dec):
    b = bin(int(_dec))[2:]
    final_b = [int(item) for item in b]
    # Padding with zeros to the rewuired length
    if len(final_b)<_len:
        final_b = np.concatenate((np.zeros((_len-len(final_b),)),np.array(final_b)))
    else:
        final_b = np.array(final_b)
    #print("Cstate:",_dec,"Bstate:",final_b)
    return final_b

def quantum_state(state_tuple):
	channel_num = len(state_tuple[0])
	state = np.array(state_tuple[0])
	state = state.argmax()
	env_time = state_tuple[1]
	state_ind = state * channel_num + env_time%channel_num
	
	return encoding(channel_num, state_ind)

#encoding(4,2)

# initializing the Quantum Device
device = qml.device('default.qubit',wires=4)
#device  = qml.device('

# Preparation of Quantum States
def state_prep(a):
    
    # a - The fixed length binary represntation of the classical state 
    # is used to represent the quantum states using the computational basis
    
    for i in range(len(a)):
        qml.RX(np.pi*a[i],wires=i)
        qml.RZ(np.pi*a[i],wires=i)

"""### Building Variation Quantum Circuits"""

# One single layer of VQC

def layers(W):
    
    # W - the weight array 
    # is used as wights for this layer
    
    # Entangling the qubits
    qml.CNOT(wires=[0,1])
    qml.CNOT(wires=[1,2])
    qml.CNOT(wires=[2,3])
    #qml.CNOT(wires=[3,0])
    
    # Rotation of each qubit accordint to the weights
    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)
    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)
    qml.Rot(W[2, 0], W[2, 1], W[2, 2], wires=2)
    qml.Rot(W[3, 0], W[3, 1], W[3, 2], wires=3)

#Creating the quantum node

@qml.qnode(device, interface='torch')

def circuit(weights,angles=None):
    # Preparing the quantum states using the binary representation of classical states passed as angles
    state_prep(angles)
    
    # creating the layers
    for W in weights:
        layers(W)
        
    # Calculating the expectation values of the VQC (four qubits)
    return [qml.expval(qml.PauliZ(i)) for i in range(4)]

# VQC ciruit 
def VQC(var_q_circuit,var_q_bias,angles=None):
    
    # var_q_circuits,var_q_bias contains the weights and the bias of the network respectively
    # angles is the fixed length binary representation of the classical states
    
    weights = var_q_circuit
    
    # creating the VQC and storing the expectation values of the four qubits 
    raw_output = circuit(weights,angles=angles)+var_q_bias
    
    #drawer = qml.draw(circuit, show_all_wires=True) #, wire_order=[2,1,0,3])
    #print(drawer(weights,angles=angles))
    
    return raw_output

"""### Loss calculation and Optimisation"""

# Square Loss calculation
def square_loss(labels,predictions):
    # 
    loss = 0
    for l, p in zip(labels, predictions):
        loss = loss + (l - p) ** 2
    loss = loss / len(labels)
    #print("Loss:",loss)
    return loss

# Calculating the loss for a batch of sample results
def cost(var_q_circuit,var_q_bias,features,labels):
    # features - sample batch from replay memory
    # predicting the expectation values of the main network
    predictions = [ VQC( var_q_circuit=var_q_circuit, var_q_bias=var_q_bias, angles=quantum_state(item.state))[item.action] for item in features]
    return square_loss(labels,predictions)

# Trade-off betwenn exploration and exploitation
def exp_expl(var_q_circuit,var_q_bias,epsilon,n_actions,s,train=False):
    # Choosing the action by exploration if the epsilon value 
    if train or np.random.rand() < ((epsilon/n_actions)+(1-epsilon)):
        action = torch.argmax(VQC(var_q_circuit=var_q_circuit,var_q_bias=var_q_bias,angles=quantum_state(s)))
    else :
        action = torch.tensor(np.random.randint(0,n_actions))
    #print("Action:",action)
    return action

iter_index=[]
iter_reward=[]
iter_total_steps=[]
cost_list = []
timestep_reward=[]

#env=Radio()

#env.step(1)
#env.status

import numpy as np

class Radio():
	def __init__(self, channel = 4, max_env_steps = 100):
		"""
		Parameters
		----------
		action :
		Returns
		-------
		ob, reward, episode_over, info : tuple
			ob (object) :
				an environment-specific object representing your observation of
				the environment.
			reward (float) :
				amount of reward achieved by the previous action. The scale
				varies between environments, but the goal is always to increase
				your total reward.
			episode_over (bool) :
				whether it's time to reset the environment again. Most (but not
				all) tasks are divided up into well-defined episodes, and done
				being True indicates the episode has terminated. (For example,
				perhaps the pole tipped too far, or you lost your last life.)
			info (dict) :
				 diagnostic information useful for debugging. It can sometimes
				 be useful for learning (for example, it might contain the raw
				 probabilities behind the environment's last state change).
				 However, official evaluations of your agent are not allowed to
				 use this for learning.
		"""


		self.donecnt = 0
		self.status = np.zeros(channel, dtype=np.int).tolist()
		print("Initial the Radio Env with",channel,  "channels -> Obs in clean channels:", self.status)
		self.t = -1
		self.channel = channel
		self.max_env_steps = max_env_steps

		
	def step(self, action):
		use_ch = (self.t) % self.channel
		tmp_s = np.zeros(self.channel, dtype=np.int).tolist()
		tmp_s[use_ch] = 1
		self.status = tmp_s
		self.t -= 1
		self.take_action(action)
		reward = self.get_reward(action)
		ob = self.get_state()

		if (self.donecnt == 3 or self.t >= self.max_env_steps):
			episode_over = True
		else:
			episode_over = False
		
		return ob, reward, episode_over

#     def reset(self):
#         pass

	def get_state(self):
		return self.status

	def take_action(self, action):
		if self.status[action] == 1:
			self.donecnt+=1
			
	def get_reward(self, action):
		""" Reward is given for. """
		if self.status[action] == 1:
			return -1
		else:
			return 1

"""### Deep Q Learning & main function"""

import pandas as pd



def deep_q_learning(alpha,gamma,epsilon,edisodes,max_steps,n_tests,render=False,test=False):
    #env = gym.make('Deterministic-4x4-FrozenLake-v0')
    env=Radio()
    #n_states,n_actions = env.observation_space.n,env.action_space.n
    n_actions = 4
    #print("Number of states: ",n_states)
    print("Number of actinos: ",n_actions)
    
    num_qubits = 4
    num_layers = 2
    
    var_init_circuit = Variable(torch.tensor(0.01*np.random.randn(num_layers,num_qubits,3),device='cpu').type(torch.DoubleTensor),requires_grad=True)
    var_init_bias = Variable(torch.tensor([0.0,0.0,0.0,0.0],device='cpu').type(torch.DoubleTensor),requires_grad=True)
    
    var_q_circuit = var_init_circuit
    var_q_bias = var_init_bias
    
    print("\n INITIAL MAIN CIRCUIT:")
    drawer = qml.draw(circuit)
    print(drawer(var_q_circuit,[0,0,0,0]))
    
    # Creating target network circuit from the main network
    var_target_q_circuit = var_q_circuit.clone().detach()
    var_target_q_bias = var_q_bias.clone().detach()
    
    # Optimzation using RMSprop
    opt = torch.optim.RMSprop([var_q_circuit,var_q_bias],lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
    
    target_update = 20
    batch_size=5
    optimize_steps=5
    
    target_update_counter = 0
    
    
    memory = Replay_mem(80)
    
    
    for episode in range(episodes):
        #print("\nEpisode: ",episode)
        if episode %10 ==0:
            print("\nEpisode: ",episode)
        s = ([0,0,0,0],0)
        env.donecnt=0
        env.t=0
        a=exp_expl(var_q_circuit=var_q_circuit,var_q_bias=var_q_bias,epsilon=epsilon,n_actions=n_actions,s=s).item()
        #print("a:",a)
        t=0
        total_reward=0
        done=False
        
        
        while t<max_steps:
            #if render:
            #    print("###Render###")
            #    env.render()
            #    print("###Render###")
            t+=1
            target_update_counter+=1
            
            
            s_,reward,done = env.step(a)
            #print(s_,reward,done,env.donecnt)
            env_time = env.t
            s_ = np.array(s_)
            s_ = (s_,env_time)
            #print("state:reward:done:info",s_,reward,done,info)
            total_reward+=reward
            #print(total_reward)
            
            #Next action is chosen
            a_ = exp_expl(var_q_circuit=var_q_circuit,var_q_bias=var_q_bias,epsilon=epsilon,n_actions=n_actions,s=s_).item()
            
            memory.add(s,a,reward,s_,done)
            
            if len(memory) > batch_size:
                batch_sampled = memory.sample(batch_size=batch_size)
                q_target = [item.reward + (1 - int(item.done)) * gamma * torch.max(VQC(var_q_circuit = var_target_q_circuit, var_q_bias = var_target_q_bias, angles=quantum_state(item.next_state))) for item in batch_sampled]
                
                def closure():
                    opt.zero_grad()
                    loss = cost(var_q_circuit = var_q_circuit, var_q_bias = var_q_bias, features = batch_sampled, labels = q_target)
                    loss.backward()
                    return loss
                opt.step(closure)
                
                current_replay_mem = memory.disp()
                current_target_for_replay_mem = [item.reward + (1 - int(item.done)) * gamma * torch.max(VQC(var_q_circuit = var_target_q_circuit, var_q_bias = var_target_q_bias, angles=quantum_state(item.next_state))) for item in current_replay_mem]
                
            if target_update_counter>target_update:
                #print("Updating target circuit:")
                var_target_q_circuit = var_q_circuit.clone().detach()
                var_target_q_bias = var_q_bias.clone().detach()
                target_update_counter=0
            s,a = s_,a_
            epsilon = epsilon*0.99
            if done:
                #if render:
                 #   print("###Final Render###")
                  #  env.render()
                   # print("###Final Render###")
                    #print("This episode reward",total_reward)
                epsilon = epsilon/((epsilon/100)+1)
                #print("This episode reward: ",total_reward)
                timestep_reward.append(total_reward)
                iter_index.append(episode)
                iter_reward.append(total_reward)
                iter_total_steps.append(t)
                break
        #if episode%10==0:
        print("\nEpisode: ",episode)
        print("Rewars:",total_reward)
            #df = pd.DataFrame([timestep_reward, iter_index, iter_reward, iter_total_steps, var_q_circuit, var_q_bias])
            #df.to_csv("Results")
    return timestep_reward, iter_index, iter_reward, iter_total_steps, var_q_circuit, var_q_bias

if __name__=='__main__':
    alpha =0.4
    gamma=0.999
    epsilon=1.
    episodes=500
    max_steps = 100
    n_tests=2
    timestep_reward_f, iter_index_f, iter_reward_f, iter_total_steps_f , var_q_circuit, var_q_bias = deep_q_learning(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = False)

pip install pygame

def plotTrainingResultReward(_iter_index, _iter_reward, _iter_total_steps, _fileTitle):
    fig, ax = plt.subplots()
    fig.set_figwidth(20)
    fig.set_figheight(5)
    # plt.yscale('log')
    ax.plot(_iter_index, _iter_reward, '-b', label='Reward')
    # ax.plot(_iter_index, _iter_total_steps, '-r', label='Total Steps')
    leg = ax.legend();

    ax.set(xlabel='Iteration Index', 
           title=_fileTitle)
    fig.savefig(_fileTitle + "_REWARD" + "_"+ datetime.now().strftime("NO%Y%m%d%H%M%S") + ".png")

import matplotlib.pyplot as plt
from datetime import datetime
import pickle
import pygame

file_title = 'Type_3_VQDQN_CognitiveRadio_Epsilon_RMSProp' + datetime.now().strftime("NO%Y%m%d%H%M%S")

plotTrainingResultReward(_iter_index = iter_index[:], _iter_reward = iter_reward[:], _iter_total_steps = iter_total_steps[:], _fileTitle = 'Type_3Quantum_cognitiveRaio_Dynamic_Epsilon_RMSProp')

file_title = '3_VQDQN_CognitiveRadio_Epsilon_RMSProp' + datetime.now().strftime("NO%Y%m%d%H%M%S")

plotTrainingResultReward(_iter_index = iter_index, _iter_reward = iter_reward, _iter_total_steps = iter_total_steps, _fileTitle = '3_VQDQN_CognitiveRadio_Epsilon_RMSProp')

file_title = 'VQDQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp' + datetime.now().strftime("NO%Y%m%d%H%M%S")

plotTrainingResultReward(_iter_index = iter_index, _iter_reward = iter_reward, _iter_total_steps = iter_total_steps, _fileTitle = 'Quantum_DQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp')

"""# Rough WOrk"""

#def deep_q_learning(alpha,gamma,epsilon,edisodes,max_steps,n_tests,render=False,test=False):
#env = gym.make('Deterministic-4x4-FrozenLake-v0')
env=ShortestPathFrozenLake()
n_states,n_actions = env.observation_space.n,env.action_space.n
print("Number of states: ",n_states)
print("Number of actinos: ",n_actions)
    
num_qubits = 4
num_layers = 2
    
    #var_init_circuit = Variable(torch.tensor(0.01*np.random.randn(num_layers,num_qubits,3),device='cpu').type(torch.DoubleTensor),requires_grad=True)
    #var_init_bias = Variable(torch.tensor([0.0,0.0,0.0,0.0],device='cpu').type(torch.DoubleTensor),requires_grad=True)
    
    #var_q_circuit = var_init_circuit
    #var_q_bias = var_init_bias
    
var_target_q_circuit = var_q_circuit.clone().detach()
var_target_q_bias = var_q_bias.clone().detach()
opt = torch.optim.RMSprop([var_q_circuit,var_q_bias],lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
    
target_update = 20
batch_size=5
optimize_steps=5
   # 
target_update_counter = 0
    
    
memory = Replay_mem(80)
render= False    
episodes = 10000
for episode in range(2501,episodes):
    #print("\nEpisode: ",episode)
    
    s = env.reset()
    a=exp_expl(var_q_circuit=var_q_circuit,var_q_bias=var_q_bias,epsilon=epsilon,n_actions=n_actions,s=s).item()
    #print("a:",a)
    t=0
    total_reward=0
    done=False


    while t<max_steps:
        if render:
            print("###Render###")
            env.render()
            print("###Render###")
        t+=1
        target_update_counter+=1
        s_,reward,done,info = env.step(a)
        #print("state:reward:done:info",s_,reward,done,info)
        total_reward+=reward
        #print(total_reward)
        a_=exp_expl(var_q_circuit=var_q_circuit,var_q_bias=var_q_bias,epsilon=epsilon,n_actions=n_actions,s=s_).item()
        memory.add(s,a,reward,s_,done)

        if len(memory) > batch_size:
            batch_sampled = memory.sample(batch_size=batch_size)
            q_target = [item.reward + (1 - int(item.done)) * gamma * torch.max(VQC(var_q_circuit = var_target_q_circuit, var_q_bias = var_target_q_bias, angles=encoding(4,item.next_state))) for item in batch_sampled]

            def closure():
                opt.zero_grad()
                loss = cost(var_q_circuit = var_q_circuit, var_q_bias = var_q_bias, features = batch_sampled, labels = q_target)
                loss.backward()
                return loss
            opt.step(closure)

            current_replay_mem = memory.disp()
            current_target_for_replay_mem = [item.reward + (1 - int(item.done)) * gamma * torch.max(VQC(var_q_circuit = var_target_q_circuit, var_q_bias = var_target_q_bias, angles=encoding(4,item.next_state))) for item in current_replay_mem]

        if target_update_counter>target_update:
            #print("Updating target circuit:")
            var_target_q_circuit = var_q_circuit.clone().detach()
            var_target_q_bias = var_q_bias.clone().detach()
            target_update_counter=0
        s,a = s_,a_

        if done:
            if render:
                print("###Final Render###")
                env.render()
                print("###Final Render###")
                print("This episode reward",total_reward)
            epsilon = epsilon/((epsilon/100)+1)
            #print("This episode reward: ",total_reward)
            timestep_reward.append(total_reward)
            iter_index.append(episode)
            iter_reward.append(total_reward)
            iter_total_steps.append(t)
            break
    if episode %10 ==0:
        print("\nEpisode: ",episode)
        print("Rewars:",total_reward)
#return timestep_reward, iter_index, iter_reward, iter_total_steps, var_q_circuit, var_q_bias

file_title = 'VQDQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp' + datetime.now().strftime("NO%Y%m%d%H%M%S")

plotTrainingResultReward(_iter_index = iter_index, _iter_reward = iter_reward, _iter_total_steps = iter_total_steps, _fileTitle = 'Quantum_DQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp')





import pygame
pygame.init()

import gym
from gym.envs import toy_text

class ShortestPathFrozenLake(toy_text.frozen_lake.FrozenLakeEnv):
    def __init__(self, **kwargs):
        super(ShortestPathFrozenLake, self).__init__(**kwargs)

        for state in range(16): # for all states
            for action in range(4): # for all actions
                my_transitions = []
                for (prob, next_state, _, is_terminal) in self.P[state][action]:
                    row = next_state // self.ncol
                    col = next_state - row * self.ncol
                    tile_type = self.desc[row, col]
                    if tile_type == b'H':
                        reward = -0.2
                    elif tile_type == b'G':
                        reward = 1.
                    else:
                        reward = -0.01

                    my_transitions.append((prob, next_state, reward, is_terminal))
                self.P[state][action] = my_transitions

pip install pygame

device = qml.device('default.qubit',wires=1)

@qml.qnode(device)
def circuit(params):
    qml.RX(params[0],wires=0)
    qml.RY(params[1],wires=0)
    return qml.expval(qml.PauliZ(0))

print(circuit([0.54,0.12]))

dcircuit = qml.grad(circuit,argnum=0)

print(dcircuit([0.54,0.12]))

