# -*- coding: utf-8 -*-
"""QVC_FrozenLake (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZGWcLb2U7gMrK17Ytpv916J8LbV5l8CJ
"""

# Importing standard Qiskit libraries
from qiskit import QuantumCircuit, transpile, Aer, IBMQ
from qiskit.tools.jupyter import *
from qiskit.visualization import *
from ibm_quantum_widgets import *
from qiskit.providers.aer import QasmSimulator

# Loading your IBM Quantum account(s)
provider = IBMQ.load_account()

pip install autoray==0.2.5

pip install gym

pip install pennylane

# Differentiable Quantum programming using pennylane
import pennylane as qml 
from pennylane import numpy as np

import gym





import torch
import torch.nn as nn
from torch.autograd import Variable
from collections import namedtuple
import random

from gym.envs.registration import register
'''register(
     id='Deterministic-4x4-FrozenLake-v0', # name given to this new environment
     entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', # env entry point
     kwargs={'map_name': '4x4', 'is_slippery': False} # argument passed to the env
 )'''
register(
    id='Deterministic-ShortestPath-4x4-FrozenLake-v0', # name given to this new environment
    entry_point='ShortestPathFrozenLake:ShortestPathFrozenLake', # env entry point
    kwargs={'map_name': '4x4', 'is_slippery': False} # argument passed to the env
)

"""### Replay Memory"""

# Creating Replay memory to store the ('state','action','reward','next_state','done')

Transition = namedtuple('Transition',('state','action','reward','next_state','done'))

class Replay_mem(object):
    def __init__(self,n):
        self.n = n
        self.mem = []
        self.pos = 0
    
    def add(self,*args):
        if len(self.mem)<self.n:
            self.mem.append(None)
        self.mem[self.pos]=Transition(*args)
        self.pos=(self.pos+1)%self.n
        
    def sample(self,batch_size):
        return random.sample(self.mem,batch_size)
    
    def disp(self):
        return self.mem
    
    def __len__(self):
        return len(self.mem)

"""### Embedding Classical states to Qubits using " Basis Embedding """"

# Converting the state(in decimal) to fixed length binary representation
def encoding(_len,_dec):
    b = bin(int(_dec))[2:]
    final_b = [int(item) for item in b]
    # Padding with zeros to the rewuired length
    if len(final_b)<_len:
        final_b = np.concatenate((np.zeros((_len-len(final_b),)),np.array(final_b)))
    else:
        final_b = np.array(final_b)
    print("Cstate:",_dec,"Bstate:",final_b)
    return final_b

#encoding(4,2)

# initializing the Quantum Device
device = qml.device('default.qubit',wires=4)

# Preparation of Quantum States
def state_prep(a):
    
    # a - The fixed length binary represntation of the classical state 
    # is used to represent the quantum states using the computational basis
    
    for i in range(len(a)):
        qml.RX(np.pi*a[i],wires=i)
        qml.RZ(np.pi*a[i],wires=i)

"""### Building Variation Quantum Circuits"""

# One single layer of VQC

def layers(W):
    
    # W - the weight array 
    # is used as wights for this layer
    
    # Entangling the qubits
    qml.CNOT(wires=[0,1])
    qml.CNOT(wires=[1,2])
    qml.CNOT(wires=[2,3])
    #qml.CNOT(wires=[3,0])
    
    # Rotation of each qubit accordint to the weights
    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)
    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)
    qml.Rot(W[2, 0], W[2, 1], W[2, 2], wires=2)
    qml.Rot(W[3, 0], W[3, 1], W[3, 2], wires=3)

#Creating the quantum node

@qml.qnode(device, interface='torch')

def circuit(weights,angles=None):
    # Preparing the quantum states using the binary representation of classical states passed as angles
    state_prep(angles)
    
    # creating the layers
    for W in weights:
        layers(W)
        
    # Calculating the expectation values of the VQC (four qubits)
    return [qml.expval(qml.PauliZ(i)) for i in range(4)]

# VQC ciruit 
def VQC(var_q_circuit,var_q_bias,angles=None):
    
    # var_q_circuits,var_q_bias contains the weights and the bias of the network respectively
    # angles is the fixed length binary representation of the classical states
    
    weights = var_q_circuit
    
    # creating the VQC and storing the expectation values of the four qubits 
    raw_output = circuit(weights,angles=angles)+var_q_bias
    
    #drawer = qml.draw(circuit, show_all_wires=True) #, wire_order=[2,1,0,3])
    #print(drawer(weights,angles=angles))
    
    return raw_output

"""### Loss calculation and Optimisation"""

# Square Loss calculation
def square_loss(labels,predictions):
    # 
    loss = 0
    for l, p in zip(labels, predictions):
        loss = loss + (l - p) ** 2
    loss = loss / len(labels)
    print("Loss:",loss)
    return loss

# Calculating the loss for a batch of sample results
def cost(var_q_circuit,var_q_bias,features,labels):
    # features - sample batch from replay memory
    # predicting the expectation values of the main network
    predictions = [ VQC( var_q_circuit=var_q_circuit, var_q_bias=var_q_bias, angles=encoding(4,item.state))[item.action] for item in features]
    return square_loss(labels,predictions)

# Trade-off betwenn exploration and exploitation
def exp_expl(var_q_circuit,var_q_bias,epsilon,n_actions,s,train=False):
    # Choosing the action by exploration if the epsilon value 
    if train or np.random.rand() < ((epsilon/n_actions)+(1-epsilon)):
        action = torch.argmax(VQC(var_q_circuit=var_q_circuit,var_q_bias=var_q_bias,angles=encoding(4,s)))
    else :
        action = torch.tensor(np.random.randint(0,n_actions))
    #print("Action:",action)
    return action

iter_index=[]
iter_reward=[]
iter_total_steps=[]
cost_list = []
timestep_reward=[]

import gym
from gym.envs import toy_text

class ShortestPathFrozenLake(toy_text.frozen_lake.FrozenLakeEnv):
    def __init__(self, **kwargs):
        super(ShortestPathFrozenLake, self).__init__(**kwargs)

        for state in range(16): # for all states
            for action in range(4): # for all actions
                my_transitions = []
                for (prob, next_state, _, is_terminal) in self.P[state][action]:
                    row = next_state // self.ncol
                    col = next_state - row * self.ncol
                    tile_type = self.desc[row, col]
                    if tile_type == b'H':
                        reward = -0.2
                    elif tile_type == b'G':
                        reward = 1.
                    else:
                        reward = -0.01

                    my_transitions.append((prob, next_state, reward, is_terminal))
                self.P[state][action] = my_transitions

"""Deep Q Learning Function"""

def deep_q_learning(alpha,gamma,epsilon,edisodes,max_steps,n_tests,render=False,test=False):
    #env = gym.make('Deterministic-4x4-FrozenLake-v0')
    env=ShortestPathFrozenLake()
    n_states,n_actions = env.observation_space.n,env.action_space.n
    print("Number of states: ",n_states)
    print("Number of actinos: ",n_actions)
    
    num_qubits = 4
    num_layers = 2
    
    var_init_circuit = Variable(torch.tensor(0.01*np.random.randn(num_layers,num_qubits,3),device='cpu').type(torch.DoubleTensor),requires_grad=True)
    var_init_bias = Variable(torch.tensor([0.0,0.0,0.0,0.0],device='cpu').type(torch.DoubleTensor),requires_grad=True)
    
    var_q_circuit = var_init_circuit
    var_q_bias = var_init_bias
    
    print("\n INITIAL MAIN CIRCUIT:")
    drawer = qml.draw(circuit, show_all_wires=True)
    print(drawer(var_q_circuit,[0,0,0,0]))
    
    var_target_q_circuit = var_q_circuit.clone().detach()
    var_target_q_bias = var_q_bias.clone().detach()
    opt = torch.optim.RMSprop([var_q_circuit,var_q_bias],lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
    
    target_update = 20
    batch_size=5
    optimize_steps=5
    
    target_update_counter = 0
    
    
    memory = Replay_mem(80)
    
    
    for episode in range(episodes):
        #print("\nEpisode: ",episode)
        if episode %10 ==0:
            print("\nEpisode: ",episode)
        s = env.reset()
        a=exp_expl(var_q_circuit=var_q_circuit,var_q_bias=var_q_bias,epsilon=epsilon,n_actions=n_actions,s=s).item()
        #print("a:",a)
        t=0
        total_reward=0
        done=False
        
        
        while t<max_steps:
            if render:
                print("###Render###")
                env.render()
                print("###Render###")
            t+=1
            target_update_counter+=1
            s_,reward,done,info = env.step(a)
            #print("state:reward:done:info",s_,reward,done,info)
            total_reward+=reward
            #print(total_reward)
            a_=exp_expl(var_q_circuit=var_q_circuit,var_q_bias=var_q_bias,epsilon=epsilon,n_actions=n_actions,s=s_).item()
            memory.add(s,a,reward,s_,done)
            
            if len(memory) > batch_size:
                batch_sampled = memory.sample(batch_size=batch_size)
                q_target = [item.reward + (1 - int(item.done)) * gamma * torch.max(VQC(var_q_circuit = var_target_q_circuit, var_q_bias = var_target_q_bias, angles=encoding(4,item.next_state))) for item in batch_sampled]
                
                def closure():
                    opt.zero_grad()
                    loss = cost(var_q_circuit = var_q_circuit, var_q_bias = var_q_bias, features = batch_sampled, labels = q_target)
                    loss.backward()
                    return loss
                opt.step(closure)
                
                current_replay_mem = memory.disp()
                current_target_for_replay_mem = [item.reward + (1 - int(item.done)) * gamma * torch.max(VQC(var_q_circuit = var_target_q_circuit, var_q_bias = var_target_q_bias, angles=encoding(4,item.next_state))) for item in current_replay_mem]
                
            if target_update_counter>target_update:
                #print("Updating target circuit:")
                var_target_q_circuit = var_q_circuit.clone().detach()
                var_target_q_bias = var_q_bias.clone().detach()
                target_update_counter=0
            s,a = s_,a_
            
            if done:
                if render:
                    print("###Final Render###")
                    env.render()
                    print("###Final Render###")
                    print("This episode reward",total_reward)
                epsilon = epsilon/((epsilon/100)+1)
                #print("This episode reward: ",total_reward)
                timestep_reward.append(total_reward)
                iter_index.append(episode)
                iter_reward.append(total_reward)
                iter_total_steps.append(t)
                break
        if episode %10 ==0:
            print("\nEpisode: ",episode)
            print("Rewars:",total_reward)
    return timestep_reward, iter_index, iter_reward, iter_total_steps, var_q_circuit, var_q_bias

if __name__=='__main__':
    alpha =0.4
    gamma=0.999
    epsilon=1.
    episodes=7500
    max_steps = 2500
    n_tests=2
    timestep_reward_f, iter_index_f, iter_reward_f, iter_total_steps_f , var_q_circuit, var_q_bias = deep_q_learning(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = False)

pip install pygame

def plotTrainingResultReward(_iter_index, _iter_reward, _iter_total_steps, _fileTitle):
    fig, ax = plt.subplots()
    fig.set_figwidth(20)
    fig.set_figheight(5)
    # plt.yscale('log')
    ax.plot(_iter_index, _iter_reward, '-b', label='Reward')
    # ax.plot(_iter_index, _iter_total_steps, '-r', label='Total Steps')
    leg = ax.legend();

    ax.set(xlabel='Iteration Index', 
           title=_fileTitle)
    fig.savefig(_fileTitle + "_REWARD" + "_"+ datetime.now().strftime("NO%Y%m%d%H%M%S") + ".png")

import matplotlib.pyplot as plt
from datetime import datetime
import pickle
import pygame

file_title = 'VQDQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp' + datetime.now().strftime("NO%Y%m%d%H%M%S")

plotTrainingResultReward(_iter_index = iter_index, _iter_reward = iter_reward, _iter_total_steps = iter_total_steps, _fileTitle = 'Quantum_DQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp')

file_title = 'VQDQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp' + datetime.now().strftime("NO%Y%m%d%H%M%S")

plotTrainingResultReward(_iter_index = iter_index, _iter_reward = iter_reward, _iter_total_steps = iter_total_steps, _fileTitle = 'Quantum_DQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp')

file_title = 'VQDQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp' + datetime.now().strftime("NO%Y%m%d%H%M%S")

plotTrainingResultReward(_iter_index = iter_index, _iter_reward = iter_reward, _iter_total_steps = iter_total_steps, _fileTitle = 'Quantum_DQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp')

"""# Rough WOrk"""

#def deep_q_learning(alpha,gamma,epsilon,edisodes,max_steps,n_tests,render=False,test=False):
#env = gym.make('Deterministic-4x4-FrozenLake-v0')
env=ShortestPathFrozenLake()
n_states,n_actions = env.observation_space.n,env.action_space.n
print("Number of states: ",n_states)
print("Number of actinos: ",n_actions)
    
num_qubits = 4
num_layers = 2
    
    #var_init_circuit = Variable(torch.tensor(0.01*np.random.randn(num_layers,num_qubits,3),device='cpu').type(torch.DoubleTensor),requires_grad=True)
    #var_init_bias = Variable(torch.tensor([0.0,0.0,0.0,0.0],device='cpu').type(torch.DoubleTensor),requires_grad=True)
    
    #var_q_circuit = var_init_circuit
    #var_q_bias = var_init_bias
    
var_target_q_circuit = var_q_circuit.clone().detach()
var_target_q_bias = var_q_bias.clone().detach()
opt = torch.optim.RMSprop([var_q_circuit,var_q_bias],lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
    
target_update = 20
batch_size=5
optimize_steps=5
   # 
target_update_counter = 0
    
    
memory = Replay_mem(80)
render= False    
episodes = 10000
for episode in range(2501,episodes):
    #print("\nEpisode: ",episode)
    
    s = env.reset()
    a=exp_expl(var_q_circuit=var_q_circuit,var_q_bias=var_q_bias,epsilon=epsilon,n_actions=n_actions,s=s).item()
    #print("a:",a)
    t=0
    total_reward=0
    done=False


    while t<max_steps:
        if render:
            print("###Render###")
            env.render()
            print("###Render###")
        t+=1
        target_update_counter+=1
        s_,reward,done,info = env.step(a)
        #print("state:reward:done:info",s_,reward,done,info)
        total_reward+=reward
        #print(total_reward)
        a_=exp_expl(var_q_circuit=var_q_circuit,var_q_bias=var_q_bias,epsilon=epsilon,n_actions=n_actions,s=s_).item()
        memory.add(s,a,reward,s_,done)

        if len(memory) > batch_size:
            batch_sampled = memory.sample(batch_size=batch_size)
            q_target = [item.reward + (1 - int(item.done)) * gamma * torch.max(VQC(var_q_circuit = var_target_q_circuit, var_q_bias = var_target_q_bias, angles=encoding(4,item.next_state))) for item in batch_sampled]

            def closure():
                opt.zero_grad()
                loss = cost(var_q_circuit = var_q_circuit, var_q_bias = var_q_bias, features = batch_sampled, labels = q_target)
                loss.backward()
                return loss
            opt.step(closure)

            current_replay_mem = memory.disp()
            current_target_for_replay_mem = [item.reward + (1 - int(item.done)) * gamma * torch.max(VQC(var_q_circuit = var_target_q_circuit, var_q_bias = var_target_q_bias, angles=encoding(4,item.next_state))) for item in current_replay_mem]

        if target_update_counter>target_update:
            #print("Updating target circuit:")
            var_target_q_circuit = var_q_circuit.clone().detach()
            var_target_q_bias = var_q_bias.clone().detach()
            target_update_counter=0
        s,a = s_,a_

        if done:
            if render:
                print("###Final Render###")
                env.render()
                print("###Final Render###")
                print("This episode reward",total_reward)
            epsilon = epsilon/((epsilon/100)+1)
            #print("This episode reward: ",total_reward)
            timestep_reward.append(total_reward)
            iter_index.append(episode)
            iter_reward.append(total_reward)
            iter_total_steps.append(t)
            break
    if episode %10 ==0:
        print("\nEpisode: ",episode)
        print("Rewars:",total_reward)
#return timestep_reward, iter_index, iter_reward, iter_total_steps, var_q_circuit, var_q_bias

file_title = 'VQDQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp' + datetime.now().strftime("NO%Y%m%d%H%M%S")

plotTrainingResultReward(_iter_index = iter_index, _iter_reward = iter_reward, _iter_total_steps = iter_total_steps, _fileTitle = 'Quantum_DQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp')





import pygame
pygame.init()

import gym
from gym.envs import toy_text

class ShortestPathFrozenLake(toy_text.frozen_lake.FrozenLakeEnv):
    def __init__(self, **kwargs):
        super(ShortestPathFrozenLake, self).__init__(**kwargs)

        for state in range(16): # for all states
            for action in range(4): # for all actions
                my_transitions = []
                for (prob, next_state, _, is_terminal) in self.P[state][action]:
                    row = next_state // self.ncol
                    col = next_state - row * self.ncol
                    tile_type = self.desc[row, col]
                    if tile_type == b'H':
                        reward = -0.2
                    elif tile_type == b'G':
                        reward = 1.
                    else:
                        reward = -0.01

                    my_transitions.append((prob, next_state, reward, is_terminal))
                self.P[state][action] = my_transitions

pip install pygame

device = qml.device('default.qubit',wires=1)

@qml.qnode(device)
def circuit(params):
    qml.RX(params[0],wires=0)
    qml.RY(params[1],wires=0)
    return qml.expval(qml.PauliZ(0))

print(circuit([0.54,0.12]))

dcircuit = qml.grad(circuit,argnum=0)

print(dcircuit([0.54,0.12]))

